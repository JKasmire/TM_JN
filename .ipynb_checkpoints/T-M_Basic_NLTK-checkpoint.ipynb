{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UKDS Logo](images/UKDS_Logos_Col_Grey_300dpi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Text-mining: Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the <a href=\"https://ukdataservice.ac.uk/\" target=_blank>UK Data Service</a> training series on *New Forms of Data for Social Science Research*. This series guides you through some of the most common and valuable new sources of data available for social science research: data collected from websites, social media platorms, text data, conducting simulations (agent based modelling), to name a few. We provide webinars, interactive notebooks containing live programming code, reading lists and more.\n",
    "\n",
    "* To access training materials for the entire series: <a href=\"https://github.com/UKDataServiceOpen/new-forms-of-data\" target=_blank>[Training Materials]</a>\n",
    "\n",
    "* To keep up to date with upcoming and past training events: <a href=\"https://ukdataservice.ac.uk/news-and-events/events\" target=_blank>[Events]</a>\n",
    "\n",
    "* To get in contact with feedback, ideas or to seek assistance: <a href=\"https://ukdataservice.ac.uk/help.aspx\" target=_blank>[Help]</a>\n",
    "\n",
    "<a href=\"https://www.research.manchester.ac.uk/portal/julia.kasmire.html\" target=_blank>Dr Julia Kasmire</a> and <a href=\"https://www.research.manchester.ac.uk/portal/diarmuid.mcdonnell.html\" target=_blank>Dr Diarmuid McDonnell</a> <br />\n",
    "UK Data Service  <br />\n",
    "University of Manchester <br />\n",
    "May 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Retrieval\" data-toc-modified-id=\"Retrieval-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Retrieval</a></span></li><li><span><a href=\"#Processing\" data-toc-modified-id=\"Processing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenisation\" data-toc-modified-id=\"Tokenisation-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Tokenisation</a></span></li><li><span><a href=\"#Remove-uppercase-letters\" data-toc-modified-id=\"Remove-uppercase-letters-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Remove uppercase letters</a></span></li><li><span><a href=\"#Spelling-correction\" data-toc-modified-id=\"Spelling-correction-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Spelling correction</a></span></li><li><span><a href=\"#Stopwords\" data-toc-modified-id=\"Stopwords-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Stopwords</a></span></li></ul></li><li><span><a href=\"#Basic-Natural-Language-Processing\" data-toc-modified-id=\"Basic-Natural-Language-Processing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Basic Natural Language Processing</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Bibliography\" data-toc-modified-id=\"Bibliography-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Bibliography</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There is a table of contents provided here at the top of the notebook, but you can also access this menu at any point by clicking the Table of Contents button on the top toolbar (an icon with four horizontal bars, if unsure hover your mouse over the buttons). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in text-mining, or any form of data-mining, is retrieving a data set to work with. Within text-mining, or any language analysis context, one data set is usually referred to as 'a corpus' while multiple data sets are referred to as 'corpora' because it is a latin word and therefore has a funny plural. \n",
    "\n",
    "For text-mining, a corpus can be:\n",
    "- a set of tweets, \n",
    "- the full text of an 18th centrury novel,\n",
    "- the contents of a page in the dictionary, \n",
    "- random gibberish letters and numbers, or\n",
    "- just about anything else in text format. \n",
    "\n",
    "\n",
    "Retrieval is a very important step, but it is not the focus of this training series. If you are particularly interested in creating a corpus from internet adat, then we recommend you check out our previous training sessions on Web-scraping (recording or jupyter notebook) and API's (recording or jupyter notebook) Both of these demonstrate and discuss ways to get data from the internet that you could use to build a corpus. \n",
    "\n",
    "Instead, for the purposes of this session, we will assume that you already have a corpus to analyse. This is easy for us to assume, because we have provided a sample text file that we can use as a corpus for these exercises. \n",
    "\n",
    "First, let's check that it is there. To do that, click in the code cell below and hit the 'Run' button at the top of this page or by holding down the 'Shift' key and hitting the 'Enter' key. \n",
    "\n",
    "For the rest of this notebook, I will use 'Run/Shift+Enter' as short hand for 'click in the code cell below and hit the 'Run' button at the top of this page or by hold down the 'Shift' key while hitting the 'Enter' key'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Succesfully imported necessary modules\n",
      "\n",
      "2. One of the files in ./data is... sample_text.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It is good practice to always start by importing the modules and packages you will need. \n",
    "# os is a module for navigating your machine (e.g., file directories).\n",
    "# nltk stands for natural language tool kit and is useful for text-mining. \n",
    "# The print statement is just a bit of encouragement!\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "\n",
    "print(\"1. Succesfully imported necessary modules\")    \n",
    "print(\"\")\n",
    "\n",
    "# List all of the files in the \"data\" folder that is provided to you\n",
    "for file in os.listdir(\"./data\"):\n",
    "   print(\"2. One of the files in ./data is...\", file)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________\n",
    "Great! We have imported a useful module and used it to check that we have access to the sample_text file. \n",
    "\n",
    "Now we need to load that sample_text file into a variable that we can work with in python. Time to Run/Shift+Enter again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample corpus. It haz some spelling errors and has numbers written two ways. For example, it has both 1972 and ninety-six. \n",
      "\n",
      "This sample corpus also uses abbreviations sometimes, but not always. California is spelled out once but also written CA. \n",
      "\n",
      "To really complicate things, another country name is written as the U.K., the UK, the United Kingdom, the United Kingdom of Great Britain and The United Kingdom of Great Britain and Northern Ireland becuase sometimes full names are important. \n",
      "\n",
      "Further, here is a bunch of unrelated toxt just to fill up the space. \n",
      "\n",
      "This privacy policy (“Privacy Policy”) is intended to inform you of some policies and practices regarding the collection, use, and disclosure of your Personal Information through our site and any other sites that links to this Privacy Policy (the “Site”). We define “Personal Information” as information that allows someone to identify you personally or contact you, including for example your name, address, telephone number, and email address.\n",
      "\n",
      "By registering with us or using our Site, you expressly consent to the collection, use, processing, and disclosure of your Personal Information in accordance with this Privacy Policy. If you reside outside the United States, you understand that your Personal Information may be processed in countries (including the United States) where laws regarding processing personal information may be less stringent than in your country.\n",
      "\n",
      "Capitalized terms used in this Privacy Policy but not defined in this Privacy Policy have the meanings given to them in the Terms of Use or Contributor Terms, as applicable.\n",
      "\n",
      "Personal Information You Provide to Us. When you register for an account on our Site, we collect your name, e-mail address, and e-mail preferences, and password for your account. You may also provide us additional information on an optional basis indicated as such in connection with your account and/or your use of our Site. When you choose to participate in a survey, we may collect your name, e-mail address, mailing address, phone number, age, gender and/or other requested information. When you give us Feedback, we may collect any information that is contained in your Feedback. When you contact us by sending us an e-mail, fax, or letter, we collect your e-mail address, fax number, or mailing address, and any information contained in the e-mail, fax, or letter you send us.\n",
      "\n",
      "Personal Information You Provide to Us Using Social Networking Connections. When you register your account with Facebook, Twitter, or other third party social networking services (each, a “Social Networking Site”), we collect basic information about you and your friends that is provided to us by the Social Networking Sites. We combine this information with other personal information we collect from the Service. If you elect to share any information with Social Networking Sites while using our Site, we will share such information with the Social Networking Site and their use of such information will be subject to their own privacy policies and not by this Privacy Policy.\n",
      "\n",
      "Personal Information Collected via Technology. As you use our Site, some information may also be collected passively, including your Internet protocol address, browser type, the website you visited before browsing our Site, pages you viewed, how long you spent on a page, browser language, and access time.\n",
      "\n",
      "Cookies. We may automatically collect information using “cookies.” Cookies are small data files stored on your hard drive by a website. Among other things, cookies help us make our Site and your experience better. We use cookies to see which parts and features of our Site are popular and to count visits to our Site.\n",
      "\n",
      "Web Beacons. We may log information using digital images called “web beacons” (also called “pixel tags”) on our Site or in our emails. We use web beacons to manage cookies, count visits, and to learn what marketing works and what does not. We also use web beacons to tell if you open or act on our emails.\n",
      "\n",
      "Analytics. We use Google Analytics and other analytics services to help analyze how users use the Site. These analytics services uses cookies to collect and store information such as how often users visit the Site, what pages they visit, and what other sites they used prior to coming to the Site. We use the information we get from these analytics services only to improve our Site and our services. Please see the following links for more information about Google Analytics: http://www.google.com/privacy.html and https://tools.google.com/dlpage/gaoptout/.\n",
      "Personal Information from Other Sources. We may receive Personal Information about you from other sources, including other users. We may associate this information with the other Personal Information we have collected about you.\n",
      "\n",
      "Use of Information\n",
      "We use your Personal Information to respond to requests that you make, to personalize your online experience on our Site, and to provide and improve our Site and its associated features and our services.\n",
      "\n",
      "We track IP protocol addresses in conjunction with session Cookies to analyze our web page flow and to keep track of your session information and clickstream data. We use persistent Cookies to keep track of your login name and password in connection with your use of, and uploading of Your Content through, our Site and to personalize your online experience on our Site.\n",
      "\n",
      "We may use your Personal Information to send you electronic newsletters or promotional e-mails, unless you have requested not to receive such promotional communications from us. We may also use your Personal Information to send you communications relating to any of our programs for which you have registered, and you may opt out of these communications if you terminate your account on our Site.\n",
      "\n",
      "We may also use your Personal Information to protect, investigate, and deter against fraudulent, unauthorized, or illegal activity.\n",
      "\n",
      "We may create Anonymous Information records from Personal Information. We reserve the right to use and disclose Anonymous Information at our discretion. “Anonymous Information” means information that is not associated with or linked to your Personal Information.This privacy policy (“Privacy Policy”) is intended to inform you of Riot New Media Group, Inc’s (“Riot New Media Group, Inc,” “we,” “us,” or “our”) policies and practices regarding the collection, use, and disclosure of your Personal Information through our site located at https://bookriot.com/ and any other sites that links to this Privacy Policy (the “Site”). We define “Personal Information” as information that allows someone to identify you personally or contact you, including for example your name, address, telephone number, and email address.\n",
      "\n",
      "By registering with us or using our Site, you expressly consent to the collection, use, processing, and disclosure of your Personal Information in accordance with this Privacy Policy. If you reside outside the United States, you understand that your Personal Information may be processed in countries (including the United States) where laws regarding processing personal information may be less stringent than in your country.\n",
      "\n",
      "Capitalized terms used in this Privacy Policy but not defined in this Privacy Policy have the meanings given to them in the Terms of Use or Contributor Terms, as applicable.\n",
      "\n",
      "Personal Information You Provide to Us. When you register for an account on our Site, we collect your name, e-mail address, and e-mail preferences, and password for your account. You may also provide us additional information on an optional basis indicated as such in connection with your account and/or your use of our Site. When you choose to participate in a survey, we may collect your name, e-mail address, mailing address, phone number, age, gender and/or other requested information. When you give us Feedback, we may collect any information that is contained in your Feedback. When you contact us by sending us an e-mail, fax, or letter, we collect your e-mail address, fax number, or mailing address, and any information contained in the e-mail, fax, or letter you send us.\n",
      "\n",
      "Personal Information You Provide to Us Using Social Networking Connections. When you register your account with Facebook, Twitter, or other third party social networking services (each, a “Social Networking Site”), we collect basic information about you and your friends that is provided to us by the Social Networking Sites. We combine this information with other personal information we collect from the Service. If you elect to share any information with Social Networking Sites while using our Site, we will share such information with the Social Networking Site and their use of such information will be subject to their own privacy policies and not by this Privacy Policy.\n",
      "\n",
      "Personal Information Collected via Technology. As you use our Site, some information may also be collected passively, including your Internet protocol address, browser type, the website you visited before browsing our Site, pages you viewed, how long you spent on a page, browser language, and access time.\n",
      "\n",
      "Cookies. We may automatically collect information using “cookies.” Cookies are small data files stored on your hard drive by a website. Among other things, cookies help us make our Site and your experience better. We use cookies to see which parts and features of our Site are popular and to count visits to our Site.\n",
      "\n",
      "Web Beacons. We may log information using digital images called “web beacons” (also called “pixel tags”) on our Site or in our emails. We use web beacons to manage cookies, count visits, and to learn what marketing works and what does not. We also use web beacons to tell if you open or act on our emails.\n",
      "\n",
      "Analytics. We use Google Analytics and other analytics services to help analyze how users use the Site. These analytics services uses cookies to collect and store information such as how often users visit the Site, what pages they visit, and what other sites they used prior to coming to the Site. We use the information we get from these analytics services only to improve our Site and our services. Please see the following links for more information about Google Analytics: http://www.google.com/privacy.html and https://tools.google.com/dlpage/gaoptout/.\n",
      "Personal Information from Other Sources. We may receive Personal Information about you from other sources, including other users. We may associate this information with the other Personal Information we have collected about you.\n",
      "\n",
      "Use of Information\n",
      "We use your Personal Information to respond to requests that you make, to personalize your online experience on our Site, and to provide and improve our Site and its associated features and our services.\n",
      "\n",
      "We track IP protocol addresses in conjunction with session Cookies to analyze our web page flow and to keep track of your session information and clickstream data. We use persistent Cookies to keep track of your login name and password in connection with your use of, and uploading of Your Content through, our Site and to personalize your online experience on our Site.\n",
      "\n",
      "We may use your Personal Information to send you electronic newsletters or promotional e-mails, unless you have requested not to receive such promotional communications from us. We may also use your Personal Information to send you communications relating to any of our programs for which you have registered, and you may opt out of these communications if you terminate your account on our Site.\n",
      "\n",
      "We may also use your Personal Information to protect, investigate, and deter against fraudulent, unauthorized, or illegal activity.\n",
      "\n",
      "We may create Anonymous Information records from Personal Information. We reserve the right to use and disclose Anonymous Information at our discretion. “Anonymous Information” means information that is not associated with or linked to your Personal Information.\n",
      "\n",
      " \n",
      "\n",
      "Disclosure of Personal Information\n",
      "Except as otherwise stated in this Privacy Policy, we do not trade, rent, or share your Personal Information with third parties, unless you ask or authorize us to do so.\n",
      "\n",
      "We may provide your Personal Information to third party service providers who work on behalf of or with us to provide our Site and features thereof. However, these service providers do not have any independent right to share this information (except pursuant to a legal requirement such as a subpoena or warrant).\n",
      "\n",
      "Although we currently do not have a parent company, any subsidiaries, joint ventures, or other companies under a common control (collectively, “Affiliates”), we may in the future. We may share some or all of your Personal Information with our Affiliates in which case we will require our Affiliates to honor this Privacy Policy.\n",
      "\n",
      "If we or our assets related to this Privacy Policy are acquired by another company, or otherwise in connection with a merger, acquisition, bankruptcy, or dissolution, that company will receive your Personal Information collected by us in accordance with this Privacy Policy.\n",
      "\n",
      "We may share your information as required by law, or with law enforcement officers acting under the color of law, or if we believe in good faith that disclosure is necessary to: (1) comply with relevant laws or to respond to subpoenas or warrants served on us; or (2) to protect and defend ours, our Affiliates,’ other users,’ or your, rights, property or safety.\n",
      "\n",
      "Your Choices Regarding Your Personal Information\n",
      "We offer you choices regarding the collection, use, and sharing of your Personal Information. When you receive promotional communications from us or our Site, you may indicate a preference to stop receiving further promotional communications from us and you will have the opportunity to “opt-out” by following the unsubscribe instructions provided in the promotional e-mail you receive.\n",
      "\n",
      "Despite your indicated e-mail preferences, we may send you administrative e-mails regarding our Site and programs, including without limitation, for example, account creation confirmation, and notices of material changes to our Terms of Use, Contributor Terms, or Privacy Policy.\n",
      "\n",
      "You may change the email address, password and preferences you submitted related to your account on the Site at any time by editing it in your account page or you may delete your account. You may request deletion or modification of your Personal Information by us, but please note that we may be required (by law or otherwise) to keep this information and not delete it (or to keep this information for a certain time, in which case we will comply with your deletion request, only after we have fulfilled such requirements). When we delete Personal Information, it will be deleted from the active database, but may remain in our archives.\n",
      "\n",
      "When you visit our Site, we and others give you the following choices about use of mechanisms for tracking, including tracking of your online activities over time and across different websites and online services by third parties. You may also render some web beacons unusable by rejecting their associated cookies. Most web browsers are set to accept cookies by default. If you prefer, you can typically remove and reject cookies from our Site with your browser settings. If you remove or reject our cookies, it could affect how our Site and Services work for you. While we and others give you the choices described in this Policy, there are many ways Web browser signals and other similar mechanisms can indicate your choice to disable tracking, and we may not be aware of or honor every mechanism.\n",
      "\n",
      "Regarding Children. We do not intentionally gather Personal Information about users who are under the age of 13.\n",
      "\n",
      "Third Party Websites, Products, Services, Content, and Links. Please be aware that the terms of our Privacy Policy do not apply to third party websites, products, services, or content or to links provided for the foregoing on our Site. Third party providers of such third party websites, products, services, or content, may collect (via tracking technologies like Cookies or web beacons) and use anonymous information regarding your interaction with the third party website, product, service, or content that they deliver and with which you interact.\n",
      "\n",
      "Online Advertisements.\n",
      "\n",
      "We use third-party advertising companies to serve ads when you visit our Web site. These companies may use cookies, web beacons, and information (such as IP address, your ISP, and the browser you use to visit our Site) about your visits to this and other Web sites in order to provide advertisements about goods and services of interest to you. Information collected may be used, among other things, to deliver advertising relevant to your interests and to better understand the usage and visitation of our Site and the other sites tracked by these third parties. This policy does not apply to, and we are not responsible for, cookies or web beacons in third party ads, and we encourage you to check the privacy policies of advertisers and/or ad services to learn about their use of cookies and other technology. If you would like more information about this practice and to know your choices about not having this information used by these companies, please see: http://www.networkadvertising.org/managing/opt_out.asp or http://www.aboutads.info/choices/.\n",
      "\n",
      "Security. We make reasonable efforts to protect your Personal information, but no company, including Riot New Media Group, Inc, can fully eliminate security risks connected to handling information on the internet.\n",
      "\n",
      "Amendment. We may change this Privacy Policy from time to time. If we make any changes to this Privacy Policy, we will change the “Updated” date above. \n",
      "\n",
      "Disclosure of Personal Information\n",
      "Except as otherwise stated in this Privacy Policy, we do not trade, rent, or share your Personal Information with third parties, unless you ask or authorize us to do so.\n",
      "\n",
      "We may provide your Personal Information to third party service providers who work on behalf of or with us to provide our Site and features thereof. However, these service providers do not have any independent right to share this information (except pursuant to a legal requirement such as a subpoena or warrant).\n",
      "\n",
      "Although we currently do not have a parent company, any subsidiaries, joint ventures, or other companies under a common control (collectively, “Affiliates”), we may in the future. We may share some or all of your Personal Information with our Affiliates in which case we will require our Affiliates to honor this Privacy Policy.\n",
      "\n",
      "If we or our assets related to this Privacy Policy are acquired by another company, or otherwise in connection with a merger, acquisition, bankruptcy, or dissolution, that company will receive your Personal Information collected by us in accordance with this Privacy Policy.\n",
      "\n",
      "We may share your information as required by law, or with law enforcement officers acting under the color of law, or if we believe in good faith that disclosure is necessary to: (1) comply with relevant laws or to respond to subpoenas or warrants served on us; or (2) to protect and defend ours, our Affiliates,’ other users,’ or your, rights, property or safety.\n",
      "\n",
      "Your Choices Regarding Your Personal Information\n",
      "We offer you choices regarding the collection, use, and sharing of your Personal Information. When you receive promotional communications from us or our Site, you may indicate a preference to stop receiving further promotional communications from us and you will have the opportunity to “opt-out” by following the unsubscribe instructions provided in the promotional e-mail you receive.\n",
      "\n",
      "Despite your indicated e-mail preferences, we may send you administrative e-mails regarding our Site and programs, including without limitation, for example, account creation confirmation, and notices of material changes to our Terms of Use, Contributor Terms, or Privacy Policy.\n",
      "\n",
      "You may change the email address, password and preferences you submitted related to your account on the Site at any time by editing it in your account page or you may delete your account. You may request deletion or modification of your Personal Information by us, but please note that we may be required (by law or otherwise) to keep this information and not delete it (or to keep this information for a certain time, in which case we will comply with your deletion request, only after we have fulfilled such requirements). When we delete Personal Information, it will be deleted from the active database, but may remain in our archives.\n",
      "\n",
      "When you visit our Site, we and others give you the following choices about use of mechanisms for tracking, including tracking of your online activities over time and across different websites and online services by third parties. You may also render some web beacons unusable by rejecting their associated cookies. Most web browsers are set to accept cookies by default. If you prefer, you can typically remove and reject cookies from our Site with your browser settings. If you remove or reject our cookies, it could affect how our Site and Services work for you. While we and others give you the choices described in this Policy, there are many ways Web browser signals and other similar mechanisms can indicate your choice to disable tracking, and we may not be aware of or honor every mechanism.\n",
      "\n",
      "Regarding Children. We do not intentionally gather Personal Information about users who are under the age of 13.\n",
      "\n",
      "Third Party Websites, Products, Services, Content, and Links. Please be aware that the terms of our Privacy Policy do not apply to third party websites, products, services, or content or to links provided for the foregoing on our Site. Third party providers of such third party websites, products, services, or content, may collect (via tracking technologies like Cookies or web beacons) and use anonymous information regarding your interaction with the third party website, product, service, or content that they deliver and with which you interact.\n",
      "\n",
      "Online Advertisements.\n",
      "\n",
      "We use third-party advertising companies to serve ads when you visit our Web site. These companies may use cookies, web beacons, and information (such as IP address, your ISP, and the browser you use to visit our Site) about your visits to this and other Web sites in order to provide advertisements about goods and services of interest to you. Information collected may be used, among other things, to deliver advertising relevant to your interests and to better understand the usage and visitation of our Site and the other sites tracked by these third parties. This policy does not apply to, and we are not responsible for, cookies or web beacons in third party ads, and we encourage you to check the privacy policies of advertisers and/or ad services to learn about their use of cookies and other technology. If you would like more information about this practice and to know your choices about not having this information used by these companies, please see: http://www.networkadvertising.org/managing/opt_out.asp or http://www.aboutads.info/choices/.\n",
      "\n",
      "Security. We make reasonable efforts to protect your Personal information, but no company, including Riot New Media Group, Inc, can fully eliminate security risks connected to handling information on the internet.\n",
      "\n",
      "Amendment. We may change this Privacy Policy from time to time. If we make any changes to this Privacy Policy, we will change the “Updated” date above.\n"
     ]
    }
   ],
   "source": [
    "# Open the \"sample_text\" file and read (import) its contents to a variable called \"corpus\"\n",
    "with open(\"./data/sample_text.txt\", \"r\") as f:\n",
    "    corpus = f.read()\n",
    "    \n",
    "    print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________\n",
    "Hmm. Not excellent literature, but it will do for our purposes. \n",
    "\n",
    "A quick look tells us that there are capital letters, contractions, punctuation, numbers as digits, numbers written out, abbreviations, and other things that, as humans, we know are equivalent but that computers do not know about. \n",
    "\n",
    "Before we go further, it helps to know what kind of variable corpus is. Run/Shift+Enter the next code block to find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________\n",
    "This tells us that 'corpus' is one very long string of text characters.  \n",
    "\n",
    "Congratulations! We are done with the retreival portion of this process. They won't all be this easy, but today, we can put our feet up. \n",
    "\n",
    "Next up... Processing, which is about cleaning, correcting, standardizing and formatting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________\n",
    "The string we have as our corpus is a good starting point, but it is not perfect. It has a bunch of errors and punctuation which need to be corrected. But even worse, it is 'one long thing' when statistical analysis works on 'lots of short things'. \n",
    "\n",
    "So, clearly, we have a few steps to go through with our raw text. \n",
    "- Tokenisation, (or splitting text into various kinds of 'short things' that can be statistically analysed).\n",
    "- Convert to lowercase (so that 'United' and 'united' are counted as the same word).\n",
    "- Spell check (should be obvious).\n",
    "- Remove punctuation from each token (so that 'u.k.' and 'uk' are counted as the same word).\n",
    "- Filter out stop words ( like 'the' or 'to' that are consistent across all English texts, so are unhelpful for text-mining). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to cut our 'one big thing' into tokens, or 'lots of little things'. In reality, this might actually be a multi-stage step because some of the data sets you will want to analyse will need to be cut into individual newspaper articles, into tweets, etc. These then, may need to be broken down further into chapters, sections, paragraphs, sentences, words, or something else. \n",
    "\n",
    "Fortunately for us, we can skip right to breaking a text into sentences and words. These are both useful tokens in their own way, so we will see how to produce both kinds. \n",
    "\n",
    "As a side note, it is good practice to create new variables whenever you manipulate an existing variable rather than write over the original. This means that you keep the original and can go back to it anytime you need to if you want to try a different manipulation or correct an error.\n",
    " \n",
    "We start by dividing our corpus into words, splitting the string into substrings every time it finds a white space (including tabs and new lines). \n",
    "\n",
    "Let's try that. But this time, let's just have a look at the first 100 things it finds instead of the entire text.\n",
    "Run/Shift+Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'corpus', '.', 'It', 'haz', 'some', 'spelling', 'errors', 'and', 'has', 'numbers', 'written', 'two', 'ways', '.', 'For', 'example', ',', 'it', 'has', 'both', '1972', 'and', 'ninety-six', '.', 'This', 'sample', 'corpus', 'also', 'uses', 'abbreviations', 'sometimes', ',', 'but', 'not', 'always', '.', 'California', 'is', 'spelled', 'out', 'once', 'but', 'also', 'written', 'CA', '.', 'To', 'really', 'complicate', 'things', ',', 'another', 'country', 'name', 'is', 'written', 'as', 'the', 'U.K.', ',', 'the', 'UK', ',', 'the', 'United', 'Kingdom', ',', 'the', 'United', 'Kingdom', 'of', 'Great', 'Britain', 'and', 'The', 'United', 'Kingdom', 'of', 'Great', 'Britain', 'and', 'Northern', 'Ireland', 'becuase', 'sometimes', 'full', 'names', 'are', 'important', '.', 'Further', ',', 'here', 'is', 'a', 'bunch', 'of', 'unrelated', 'toxt', 'just', 'to', 'fill', 'up', 'the', 'space', '.', 'This', 'privacy', 'policy', '(', '“', 'Privacy', 'Policy', '”', ')', 'is', 'intended', 'to', 'inform', 'you', 'of', 'some', 'policies', 'and', 'practices', 'regarding', 'the', 'collection', ',', 'use', ',', 'and', 'disclosure', 'of', 'your', 'Personal', 'Information', 'through', 'our', 'site', 'and', 'any', 'other', 'sites', 'that', 'links']\n",
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing word_tokenize from nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Passing the string corpus into word tokenize to be broken into words\n",
    "corpus_words = word_tokenize(corpus)\n",
    "print(corpus_words[:10])                                                  # the [:100] within the print statement says \n",
    "                                                                           # to print only the first 100 items in the list  \n",
    "print(\"...\")                                                               # the print(\"...\") just improves output readability\n",
    "type(corpus_words)                                                         # Always good to know your variable type!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right. Let's have a look. \n",
    "\n",
    "We can see that corpus_words is a list of strings. We know it is a list because it starts and ends with square brackets and we know the things in that list are surrounded by single quotes. \n",
    "\n",
    "However, we can also see that we still have some problems with spelling errors, capital letters and puctuation. For example, each full stop at the end of a sentence appears as its own token. Interestingly, 'U.K.' is all one token, despite having full stops in. Clever stuff, this tokenisation function!\n",
    "\n",
    "This 'bag of words' reduces a lot of the contextual information within the original corpus because it ignores how the words were used or in what order they originally appeared. There is a suprising amount of insight to be gained from just counting word occurrences, but it does mean that 'building' as a verb in \"He is building a diorama for a school project.\" will be counted as a the same word as 'building' as a noun in \"The building is a clear example of brutalist architecture.\" \n",
    "\n",
    "There are other kinds of analyses that you could do, if you don't want verb-building and noun-building to be counted as the same. Let's see what one might look like by running the same basic analysis again, but this time with sentence-token things instead of word-token things. \n",
    "\n",
    "Do that funky Run/Shift+Enter thing! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sample corpus.', 'It haz some spelling errors and has numbers written two ways.', 'For example, it has both 1972 and ninety-six.', 'This sample corpus also uses abbreviations sometimes, but not always.', 'California is spelled out once but also written CA.', 'To really complicate things, another country name is written as the U.K., the UK, the United Kingdom, the United Kingdom of Great Britain and The United Kingdom of Great Britain and Northern Ireland becuase sometimes full names are important.', 'Further, here is a bunch of unrelated toxt just to fill up the space.', 'This privacy policy (“Privacy Policy”) is intended to inform you of some policies and practices regarding the collection, use, and disclosure of your Personal Information through our site and any other sites that links to this Privacy Policy (the “Site”).', 'We define “Personal Information” as information that allows someone to identify you personally or contact you, including for example your name, address, telephone number, and email address.', 'By registering with us or using our Site, you expressly consent to the collection, use, processing, and disclosure of your Personal Information in accordance with this Privacy Policy.']\n",
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing sent_tokenize from nltk\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# Same again, but this time broken into sentences\n",
    "corpus_sentences = sent_tokenize(corpus)\n",
    "print(corpus_sentences[:10])                                                  # Since these are sentences instead of words, \n",
    "                                                                              # we only want the first 10 items instead of 100.\n",
    "print(\"...\")                                                                  \n",
    "type(corpus_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________\n",
    "\n",
    "Corpus_sentences is also a list of strings (starts and ends with square brackets, each item is surrounded by single quotes). \n",
    "\n",
    "The same spelling errors, capital letters and puctuation problems are here too, but the full stop at the end of each sentence is now part of the sentence. Now, we could carry on doing two separate analyses, but let's concentrate on the words for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove uppercase letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are focussing on the 'bag of words' approach, which means we don't really care about uppercase or lowercase distinctions. We want to count 'Privacy' as the same as 'privacy', rather than as two different words. \n",
    "\n",
    "We can remove all uppercase letters with a built in python command on corpus_words. Do this in the next code cell, again returning just the first 100 items instead of the whole thing. \n",
    "\n",
    "Do the Run/Shift+Enter thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sample', 'corpus', '.', 'it', 'haz', 'some', 'spelling', 'errors', 'and', 'has', 'numbers', 'written', 'two', 'ways', '.', 'for', 'example', ',', 'it', 'has', 'both', '1972', 'and', 'ninety-six', '.', 'this', 'sample', 'corpus', 'also', 'uses', 'abbreviations', 'sometimes', ',', 'but', 'not', 'always', '.', 'california', 'is', 'spelled', 'out', 'once', 'but', 'also', 'written', 'ca', '.', 'to', 'really', 'complicate', 'things', ',', 'another', 'country', 'name', 'is', 'written', 'as', 'the', 'u.k.', ',', 'the', 'uk', ',', 'the', 'united', 'kingdom', ',', 'the', 'united', 'kingdom', 'of', 'great', 'britain', 'and', 'the', 'united', 'kingdom', 'of', 'great', 'britain', 'and', 'northern', 'ireland', 'becuase', 'sometimes', 'full', 'names', 'are', 'important', '.', 'further', ',', 'here', 'is', 'a', 'bunch']\n"
     ]
    }
   ],
   "source": [
    "# You can see that I created a new variable called corpus_lower rather than edit corpus_words directly.\n",
    "# This means I can easily compare two different processes or correct something without going back and re-running earlier steps. \n",
    "\n",
    "corpus_lower = [word.lower() for word in corpus_words]\n",
    "print(corpus_lower[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________\n",
    "Great! This is another step in the right direction. \n",
    "\n",
    "If you want a bit more practice, you can copy/paste/edit the command above to create a second version that applies to corpus_sentences instead of corpus_words. It doesn't make a whole lot of sense to do this, because uppercase letters are potentially useful in an analysis that looks at sentences. But we are just playing around here, so go ahead. Knock yourself out! \n",
    "\n",
    "\n",
    "Forging ahead, let's filter out punctuation. We can define a string that includes all the standard English language punctuation, and then use that to iterate over corpus_words, removing anything that matches.\n",
    "\n",
    "But wait... Do we really want to remove the:\n",
    "- dash in 'ninety-six'? \n",
    "- full stops in 'u.k.'? \n",
    "- the apostrophe in contractions or possessives?\n",
    "\n",
    "There are no right or wrong answers here. Every project will have to decide, based on the research questions, what is the right choice for the specific context. In this case, we want to remove the full stops, even from 'u.k.' becomes identical to 'uk'. \n",
    "\n",
    "But, at the same time, we don't want to remove dashes or apostrophes because I decided that 'ninetysix' and 'whats' are not words.\n",
    "\n",
    "Run/Shift+Enter, as is tradition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!()-[]{};:'\"\\,<>./?@#$%^&*_~\n",
      "!\"#$%&()*+,./:;<=>?@[\\]^_`{|}~“”[”]\n",
      "...\n",
      "{33: None, 40: None, 41: None, 45: None, 91: None, 93: None, 123: None, 125: None, 59: None, 58: None, 39: None, 34: None, 92: None, 44: None, 60: None, 62: None, 46: None, 47: None, 63: None, 64: None, 35: None, 36: None, 37: None, 94: None, 38: None, 42: None, 95: None, 126: None}\n",
      "...\n",
      "['this', 'is', 'a', 'sample', 'corpus', '', 'it', 'haz', 'some', 'spelling', 'errors', 'and', 'has', 'numbers', 'written', 'two', 'ways', '', 'for', 'example', '', 'it', 'has', 'both', '1972', 'and', 'ninety-six', '', 'this', 'sample', 'corpus', 'also', 'uses', 'abbreviations', 'sometimes', '', 'but', 'not', 'always', '', 'california', 'is', 'spelled', 'out', 'once', 'but', 'also', 'written', 'ca', '', 'to', 'really', 'complicate', 'things', '', 'another', 'country', 'name', 'is', 'written', 'as', 'the', 'uk', '', 'the', 'uk', '', 'the', 'united', 'kingdom', '', 'the', 'united', 'kingdom', 'of', 'great', 'britain', 'and', 'the', 'united', 'kingdom', 'of', 'great', 'britain', 'and', 'northern', 'ireland', 'becuase', 'sometimes', 'full', 'names', 'are', 'important', '', 'further', '', 'here', 'is', 'a', 'bunch']\n"
     ]
    }
   ],
   "source": [
    "# First, we want to define a variable with all the punctuation to remove.\n",
    "# Print that defined variable, just to check it is correct.\n",
    "\n",
    "English_punctuation = \"!\\\"#$%&()*+,./:;<=>?@[\\]^_`{|}~“”[”]\"  \n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~”“'''\n",
    "print(punctuations)\n",
    "print(English_punctuation)                                    \n",
    "print(\"...\")\n",
    "table_punctuation = str.maketrans('','', punctuations) # The python function 'maketrans' creates a table that maps\n",
    "print(table_punctuation)                                       # the punctation marks to 'None'. Print the table to check. \n",
    "print(\"...\")                                                   # Just to be clear, '!' is 33 in Unicode, and '\\' is 34, etc.\n",
    "                                                               # 'None' is python for nothing, not a string of the word \"none\".\n",
    "    \n",
    "corpus_no_punct = [w.translate(table) for w in corpus_lower]   # Iterate over corpus_lower, turning punctuation to nothing.\n",
    "print(corpus_no_punct[:100])                                   # Print the 1st 100 items in corpus_no_punct to check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________\n",
    "Super! \n",
    "\n",
    "Do you want to try something else? How about you create a version that *does* filter out dashes and apostrophes. \n",
    "\n",
    "C'mon. You know you can do it. \n",
    "\n",
    "Take each of the steps above and copy/paste/edit them as needed. \n",
    "- Create a copy of the line that defines the English_punctuation variable and edit it to define an All_English_Punctuation variable that includes more punctuation.\n",
    "- Then create a copy of the line that defines the table_punctuation variable and have it create a table_all_punctuation variable.\n",
    "- Then create a copy of the line that creates the corpus_no_punct variable and have it create an absolutely_no_punct variable.\n",
    "- Then ask for the first 100 items of absolutely_no_punct. \n",
    "\n",
    "Feel free to change the variable names as you like. I am going for clarity, but you might prefer brevity. \n",
    "\n",
    "\n",
    "A point you might notice here... Removing the punctuation has left list items that are empty strings. Between 'corpus' and 'it', for example, is an item shown as ''. This is an empty string item. Can you think of how we might remove this? Don't worry too much about it now, because I will show you a method later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________\n",
    "Everybody loves spelling... RIGHT?!?\n",
    "\n",
    "Fortunately, there are several decent spellchecking packages written for python. They are not automatically installed and ready to import in the same way that the 'os' or 'nltk' packages were, but we just need to install the packages and import the functions we need through an installer called 'pip'. You will see 'pip' in the next code block, but since this is in jupyter notebook rather than directly in a python shell, we need to put a '!' in front of the 'pip' function. Don't worry too much about that now, I just include it here in case you find it interesting to know. \n",
    "\n",
    "The next code cell:\n",
    "- installs the 'autocorrect' package,\n",
    "- imports the Speller function, and\n",
    "- creates a one-word command that specifies that the Speller function should use English language. \n",
    "\n",
    "Run/Shift+Enter, as per usual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\programdata\\anaconda3\\lib\\site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect\n",
    "from autocorrect import Speller\n",
    "check = Speller(lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________\n",
    "Super. Creating that one-word command saves us some time, which is maybe less important here but is a good skill to be aware of if you are working on text-mining every day for weeks on end. Always be on the look out for good ways to save time. \n",
    "\n",
    "Moving on, we need to iterate over our corpus, checking and correcting each token in our punctuation free bag of words. This is easy to do if you start with a new, empty list (I called mine 'corpus_correct_spell'). As I work through corpus_no_punct, one token at a time, we append (which is just fancy for 'add to the end') the corrected word to our new blank list. \n",
    "\n",
    "Then, as usual, we have a quick look at the first 100 entries in the new 'corpus_correct_spell'. \n",
    "\n",
    "Run/Shift+Enter. You know how to do it. Don't worry if it takes a while... Checking the spelling on each word is not a cakewalk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sample', 'corpus', '', 'it', 'had', 'some', 'spelling', 'errors', 'and', 'has', 'numbers', 'written', 'two', 'ways', '', 'for', 'example', '', 'it', 'has', 'both', '1972', 'and', 'ninety-six', '', 'this', 'sample', 'corpus', 'also', 'uses', 'abbreviations', 'sometimes', '', 'but', 'not', 'always', '', 'california', 'is', 'spelled', 'out', 'once', 'but', 'also', 'written', 'ca', '', 'to', 'really', 'complicate', 'things', '', 'another', 'country', 'name', 'is', 'written', 'as', 'the', 'uk', '', 'the', 'uk', '', 'the', 'united', 'kingdom', '', 'the', 'united', 'kingdom', 'of', 'great', 'britain', 'and', 'the', 'united', 'kingdom', 'of', 'great', 'britain', 'and', 'northern', 'ireland', 'because', 'sometimes', 'full', 'names', 'are', 'important', '', 'further', '', 'here', 'is', 'a', 'bunch']\n"
     ]
    }
   ],
   "source": [
    "corpus_correct_spell = []\n",
    "\n",
    "for word in corpus_no_punct:\n",
    "    corpus_correct_spell.append(check(word))    \n",
    "\n",
    "print(corpus_correct_spell[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________\n",
    "How did it do? Well, this spell-checker replaced 'haz' with 'had' rather than 'has'. That is ok, I guess. No automatic spelling correction programme will get it 100% right 100% of the time. Maybe your project has specific research questions that won't work with this decision. \n",
    "\n",
    "In that case, you would have to check out some other spell-checkers like textblob or pyspellchecker. You might even want to custom build or adapt your own spell-checker, especially if you were working with very non-standard text, like comment boards that use a bunch of slang, common typos, or specific terms. \n",
    "\n",
    "Next up, stopwords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are typically conjunctions ('and', 'or'), prepositions ('to', 'around'), determiners ('the', 'an'), possessives ('s) and the like. The are **REALLY** common in all language, and tend to occur at about the same ratio in all kinds of writing, regardless of who did the writing or what it is about. These words are definitely important for structure as they make all the difference between \"Freeze *or* I'll shoot!\" and \"Freeze *and* I'll shoot!\". \n",
    "\n",
    "Buuuut... In the bag of words approach, these words don't have a whole lot of meaning in and of themselves. Thus, we want to remove them. \n",
    "\n",
    "Let's start by downloading the basic stopwords function built into nltk and storing the English language ones in a list called, appropriately enough, 'stop_words'. \n",
    "\n",
    "Then let's have a look at what is in that list with a print command by doing the whole Run/Shift+Enter thing in the next two (two?!?) code cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(sorted(stop_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________________________________________________________________________________________________\n",
    "Great. Now let's remove those stop_words by creating another list called corpus_no_stop_words. Then, we iterate over corpus_correct_spell, looking at them one by one and appending them to corpus_no_stop_words if and only if they do not match any of the items in the stop_words list. \n",
    "\n",
    "As you might expect, you should do the whole Run/Shift+Enter thing. Again. (I know, I know...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'corpus', '', 'spelling', 'errors', 'numbers', 'written', 'two', 'ways', '', 'example', '', '1972', 'ninety-six', '', 'sample', 'corpus', 'also', 'uses', 'abbreviations', 'sometimes', '', 'always', '', 'california', 'spelled', 'also', 'written', 'ca', '', 'really', 'complicate', 'things', '', 'another', 'country', 'name', 'written', 'uk', '', 'uk', '', 'united', 'kingdom', '', 'united', 'kingdom', 'great', 'britain', 'united', 'kingdom', 'great', 'britain', 'northern', 'ireland', 'sometimes', 'full', 'names', 'important', '', '', 'bunch', 'unrelated', 'text', 'fill', 'space', '', 'privacy', 'policy', '', '', 'privacy', 'policy', '”', '', 'intended', 'inform', 'policies', 'practices', 'regarding', 'collection', '', 'use', '', 'disclosure', 'personal', 'information', 'site', 'sites', 'links', 'privacy', 'policy', '', '', 'site', '”', '', '', 'define', '']\n"
     ]
    }
   ],
   "source": [
    "corpus_no_stop_words = []\n",
    "\n",
    "for word in corpus_correct_spell:\n",
    "    if word not in stop_words:\n",
    "        corpus_no_stop_words.append(word)\n",
    "        \n",
    "        \n",
    "print(corpus_no_stop_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________\n",
    "Hey now! That looks pretty good. \n",
    "\n",
    "But do you remember those empty strings? They are not filtered out by the stop_words because... Well. They don't match any of the items on the stop_words list. But python is aware that empty strings exist and can filter them out. \n",
    "\n",
    "Let's give it a try. Run/Shift+Enter. Do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'corpus', 'spelling', 'errors', 'numbers', 'written', 'two', 'ways', 'example', '1972', 'ninety-six', 'sample', 'corpus', 'also', 'uses', 'abbreviations', 'sometimes', 'always', 'california', 'spelled', 'also', 'written', 'ca', 'really', 'complicate', 'things', 'another', 'country', 'name', 'written', 'uk', 'uk', 'united', 'kingdom', 'united', 'kingdom', 'great', 'britain', 'united', 'kingdom', 'great', 'britain', 'northern', 'ireland', 'sometimes', 'full', 'names', 'important', 'bunch', 'unrelated', 'text', 'fill', 'space', 'privacy', 'policy', 'privacy', 'policy', '”', 'intended', 'inform', 'policies', 'practices', 'regarding', 'collection', 'use', 'disclosure', 'personal', 'information', 'site', 'sites', 'links', 'privacy', 'policy', 'site', '”', 'define', 'personal', 'information', '”', 'information', 'allows', 'someone', 'identify', 'personally', 'contact', 'including', 'example', 'name', 'address', 'telephone', 'number', 'email', 'address', 'registering', 'us', 'using', 'site', 'expressly', 'consent', 'collection']\n"
     ]
    }
   ],
   "source": [
    "corpus_no_space = list(filter(None, corpus_no_stop_words))  # This adds the empty string to the stop words list.\n",
    "\n",
    "print(corpus_no_space[:100])                           # Will we see the same result? One way to find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we also need to decide whether to dive into the text-mining or whether to carry on correcting and cleaning the text using downloaded packages that could:\n",
    "- replace contractions so 'haven't' would become 'have' and 'not',\n",
    "- replace abbreviations, so 'ca' becomes 'california' or 'uk' becomes 'united' and 'kingdom', or\n",
    "- anything else that might be specific to the text.\n",
    "\n",
    "Buuuuuuuuuuuuuut... those steps take some time and at this point, it is not clear that that time would be well spent. Let's dive in for now and we can always come back and add more correcting and cleaning steps if we decide they are needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Natural Language Processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commentary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Hopefully this chapter has demystified aspects of CSS and whetted your appetite for some applied work. The subsequent chapters provide plenty of opportunity to practice CSS with various forms of data. For now I wanted to reflect on some outstanding issues.\n",
    "\n",
    "<!-- #### Python vs R vs Julia vs ....\n",
    "\n",
    "[Perhaps a table with some properties of each?] The general point is it's your choice.\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Further Reading and Resources\n",
    "\n",
    "[Copy AQMEN reading lists] -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
