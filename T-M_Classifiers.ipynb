{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UKDS Logo](images/UKDS_Logos_Col_Grey_300dpi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Text-mining: Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the <a href=\"https://ukdataservice.ac.uk/\" target=_blank>UK Data Service</a> training series on *New Forms of Data for Social Science Research*. This series guides you through some of the most common and valuable new sources of data available for social science research: data collected from websites, social media platorms, text data, conducting simulations (agent based modelling), to name a few. We provide webinars, interactive notebooks containing live programming code, reading lists and more.\n",
    "\n",
    "* To access training materials for the entire series: <a href=\"https://github.com/UKDataServiceOpen/new-forms-of-data\" target=_blank>[Training Materials]</a>\n",
    "\n",
    "* To keep up to date with upcoming and past training events: <a href=\"https://ukdataservice.ac.uk/news-and-events/events\" target=_blank>[Events]</a>\n",
    "\n",
    "* To get in contact with feedback, ideas or to seek assistance: <a href=\"https://ukdataservice.ac.uk/help.aspx\" target=_blank>[Help]</a>\n",
    "\n",
    "<a href=\"https://www.research.manchester.ac.uk/portal/julia.kasmire.html\" target=_blank>Dr Julia Kasmire</a> and <a href=\"https://www.research.manchester.ac.uk/portal/diarmuid.mcdonnell.html\" target=_blank>Dr Diarmuid McDonnell</a> <br />\n",
    "UK Data Service  <br />\n",
    "University of Manchester <br />\n",
    "May 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Sentiment-Analysis-as-an-example-of-machine-learning/deep-learning-classification\" data-toc-modified-id=\"Sentiment-Analysis-as-an-example-of-machine-learning/deep-learning-classification-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Sentiment Analysis as an example of machine learning/deep learning classification</a></span></li><li><span><a href=\"#Analyse-trivial-documents-with-built-in-sentiment-analysis-tool\" data-toc-modified-id=\"Analyse-trivial-documents-with-built-in-sentiment-analysis-tool-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Analyse trivial documents with built-in sentiment analysis tool</a></span></li><li><span><a href=\"#Acquire-and-analyse-lell-trivial-documents\" data-toc-modified-id=\"Acquire-and-analyse-lell-trivial-documents-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Acquire and analyse lell trivial documents</a></span></li><li><span><a href=\"#Train-and-test-a-sentiment-analysis-tool-with-trivial-data\" data-toc-modified-id=\"Train-and-test-a-sentiment-analysis-tool-with-trivial-data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Train and test a sentiment analysis tool with trivial data</a></span></li><li><span><a href=\"#You-can-train-and-test-a-sentiment-analysis-tool-with-more-interesting-data-too...\" data-toc-modified-id=\"You-can-train-and-test-a-sentiment-analysis-tool-with-more-interesting-data-too...-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>You can train and test a sentiment analysis tool with more interesting data too...</a></span></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Conclusions</a></span></li><li><span><a href=\"#Further-reading\" data-toc-modified-id=\"Further-reading-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Further reading</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There is a table of contents provided here at the top of the notebook, but you can also access this menu at any point by clicking the Table of Contents button on the top toolbar (an icon with four horizontal bars, if unsure hover your mouse over the buttons). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is a commonly used example of automatic classification. To be clear, automatic classification means that a model or learning algorithm has been trained on correctly classified documents and it uses this training to returns a probability assessment of what class a new document should belong to. \n",
    "\n",
    "Sentiment analysis works the same way, but usually only has two classes - positive and negative. A trained model looks at new data and says whether that new data is likely to be positive or negative. Let's take a look!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis as an example of machine learning/deep learning classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off by importing and downloading some useful packages, including textblob. Textblob is based on nltk and has built in sentiment analysis tools. \n",
    "\n",
    "Run/Shift+Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A files we can use is...  test_min.csv\n",
      "A files we can use is...  training_min.csv\n",
      "\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "import csv                        # csv is for importing and working with csv files\n",
    "import statistics\n",
    "\n",
    "\n",
    "# List all of the files in the \"data\" folder that is provided to you\n",
    "for file in os.listdir(\"./Sentiment_Analysis\"):\n",
    "   print(\"A files we can use is... \", file)\n",
    "print(\"\")\n",
    "\n",
    "!pip install -U textblob -q\n",
    "!python -m textblob.download_corpora -q\n",
    "from textblob import TextBlob "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse trivial documents with built-in sentiment analysis tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets get some data.\n",
    "\n",
    "Run/Shift+Enter, as above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "textblob.blob.TextBlob"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Doc1 = TextBlob(\"Textblob is just super. I love it!\")              # A few simple documents in textblog format\n",
    "Doc2 = TextBlob(\"Cabbages are the worst. Say no to cabbages!\")  \n",
    "Doc3 = TextBlob(\"Paris is the capital of France. \")   \n",
    "print(\"...\")\n",
    "type(Doc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docs 1 through 3 are Textblobs, which we can see by the output of type(Doc1). \n",
    "\n",
    "We get a Textblob by passing a string to the function that we imported above. Specifically, this is done by using this format --> Textblob('string goes here'). Textblobs are ready for analysis through the textblob tools, such as the built-in sentiment analysis tool that we see in the code below. \n",
    "\n",
    "Run/Shift+Enter on those Textblobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.47916666666666663, subjectivity=0.6333333333333333)\n",
      "Sentiment(polarity=-1.0, subjectivity=1.0)\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "print(Doc1.sentiment)\n",
    "print(Doc2.sentiment)\n",
    "print(Doc3.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the previous code returns two values for each Textblob object. Polarity refers to a positive-negative spectrum while subjectivity refers to an opinion-fact spectrum. \n",
    "\n",
    "We can see, for example, that Doc1 is fairly positive but also quite subjective while Doc2 is very negative and very subjective. Doc3, in contrast, is both neutral and factual. \n",
    "\n",
    "Maybe you don't need both polarity and subjectivity. For example, if you are trying to categorise opinions, you don't need the subjectivity score and would only want the polarity. \n",
    "\n",
    "To get only one one of the two values, you can call the appropriate sub-function as shown below. \n",
    "\n",
    "Run/Shift+Enter for sub-functional fun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47916666666666663\n",
      "0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(Doc1.sentiment.polarity)\n",
    "print(Doc1.sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire and analyse lell trivial documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super. We have importand some documents (in our case, just sentences in string format) to textblob and analysed it using the built-in sentiment analyser. But we don't want to import documents one string at a time... That would take forever!\n",
    "\n",
    "Let's import data in .csv format instead! The data here comes from a set of customer reviews of amazon products. Naturally, not all of the comments in the product reviews are really on topic, but it does not actually matter for our purposes. But, I think it is only fair to warn you... There is some foul language and potentially objectionable personal opinions in the texts if you go through it all. \n",
    "\n",
    "Run/Shift+Enter (if you dare!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ï»¿\"@switchfoot http://twitpic.com/2y1zl - Awww', ' that\\'s a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"', '0'], [\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", '0'], ['@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds', '0'], ['my whole body feels itchy and like its on fire ', '0'], [\"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \", '0'], ['@Kwesidei not the whole crew ', '0'], ['Need a hug ', '0'], [\"@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\", '0'], [\"@Tatiana_K nope they didn't have it \", '0'], ['@twittera que me muera ? ', '0'], [\"spring break in plain city... it's snowing \", '0'], ['I just re-pierced my ears ', '0'], [\"@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .\", '0'], ['@octolinz16 It it counts, idk why I did either. you never talk to me anymore ', '0'], [\"@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.\", '0'], ['@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!', '0'], [\"Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?\", '0'], ['about to file taxes ', '0'], ['@LettyA ahh ive always wanted to see rent  love the soundtrack!!', '0'], ['@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks? ', '0']]\n"
     ]
    }
   ],
   "source": [
    "with open('./Sentiment_Analysis/training_min.csv', newline='') as f:              # 2, a csv of scored product reviews\n",
    "    reader = csv.reader(f)\n",
    "    Doc_set = list(reader)\n",
    "\n",
    "print(Doc_set[:20])                                                          # 3, a quick look at the first 10 items in the csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very good start (although you will see what I mean about the off-topic comments and foul language). \n",
    "\n",
    "Now, the csv has multiple strings per row, but we need to pass that to texblob to create a Textblob object before we can get a polirity or sujectivity score. \n",
    "\n",
    "The code below creates a new list that has the text string and the sentiment score for each item in the imported Doc_set, and also shows you the first 20 results of that new list to look at. \n",
    "\n",
    "Run/Shift+Enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ï»¿\"@switchfoot http://twitpic.com/2y1zl - Awww', Sentiment(polarity=0.4, subjectivity=0.9)], [\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", Sentiment(polarity=0.0, subjectivity=0.0)], ['@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds', Sentiment(polarity=0.5, subjectivity=0.5)], ['my whole body feels itchy and like its on fire ', Sentiment(polarity=0.2, subjectivity=0.4)], [\"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \", Sentiment(polarity=-0.625, subjectivity=1.0)], ['@Kwesidei not the whole crew ', Sentiment(polarity=0.2, subjectivity=0.4)], ['Need a hug ', Sentiment(polarity=0.0, subjectivity=0.0)], [\"@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\", Sentiment(polarity=0.27083333333333337, subjectivity=0.5599999999999999)], [\"@Tatiana_K nope they didn't have it \", Sentiment(polarity=0.0, subjectivity=0.0)], ['@twittera que me muera ? ', Sentiment(polarity=0.0, subjectivity=0.0)], [\"spring break in plain city... it's snowing \", Sentiment(polarity=-0.21428571428571427, subjectivity=0.35714285714285715)], ['I just re-pierced my ears ', Sentiment(polarity=0.0, subjectivity=0.0)], [\"@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .\", Sentiment(polarity=0.0, subjectivity=0.0)], ['@octolinz16 It it counts, idk why I did either. you never talk to me anymore ', Sentiment(polarity=0.0, subjectivity=0.0)], [\"@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.\", Sentiment(polarity=0.075, subjectivity=0.26666666666666666)], ['@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!', Sentiment(polarity=0.0, subjectivity=0.0)], [\"Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?\", Sentiment(polarity=0.0, subjectivity=0.0)], ['about to file taxes ', Sentiment(polarity=0.0, subjectivity=0.0)], ['@LettyA ahh ive always wanted to see rent  love the soundtrack!!', Sentiment(polarity=0.78125, subjectivity=0.6)], ['@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks? ', Sentiment(polarity=0.0, subjectivity=0.0)]]\n"
     ]
    }
   ],
   "source": [
    "Doc_set_analysed = []\n",
    "\n",
    "for item in Doc_set:\n",
    "    Doc_set_analysed.append([item[0], TextBlob(item[0]).sentiment])\n",
    "\n",
    "print(Doc_set_analysed[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try to edit the code above to return only the polarity or only the subjectivity. \n",
    "\n",
    "While you are at it, try to edit the code to return a different number of results to look at. How about 25?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test a sentiment analysis tool with trivial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in tool is all well and good, but... have a look back at the sentiment analysis scores for Doc1 and Doc2. \n",
    "- Doc1 scored as .48 on polarity, about halfway between totally neutral and totally positive. \n",
    "- Doc2 scored -1 on polarity, which is the most negative it could score. \n",
    "\n",
    "Do we really think Doc2 is so much more negative than Doc1 is positive? Hmmmm. Maybe the built-in sentiment analyser is not as accurate as we would want. Let's try to train our own, starting with a small set of trivial training and testing data sets. \n",
    "\n",
    "The following code does a few different things:\n",
    "- It defines 'train' as a data set with 10 sentences, each of which is marked as 'pos' or 'neg'.\n",
    "- It defines 'test' as a data set with 6 completely different sentences, also marked as 'pos' or 'neg'. \n",
    "- It imports NaiveBayesClassifier from the textblob.classifiers.\n",
    "- It defines 'cl' as a brand new NaiveBayesClassifier that is trained on the 'train' data set. \n",
    "\n",
    "Run/Shift+Enter to make it so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [\n",
    "    ('I love this sandwich.', 'pos'),\n",
    "    ('this is an amazing place!', 'pos'),\n",
    "    ('I feel very good about these beers.', 'pos'),\n",
    "    ('this is my best work.', 'pos'),\n",
    "    (\"what an awesome view\", 'pos'),\n",
    "    ('I do not like this restaurant', 'neg'),\n",
    "    ('I am tired of this stuff.', 'neg'),\n",
    "    (\"I can't deal with this\", 'neg'),\n",
    "    ('he is my sworn enemy!', 'neg'),\n",
    "    ('my boss is horrible.', 'neg')]\n",
    "test = [\n",
    "     ('the beer was good.', 'pos'),\n",
    "     ('I do not enjoy my job', 'neg'),\n",
    "     (\"I ain't feeling dandy today.\", 'neg'),\n",
    "     (\"I feel amazing!\", 'pos'),\n",
    "     ('Gary is a friend of mine.', 'pos'),\n",
    "     (\"I can't believe I'm doing this.\", 'neg')]\n",
    "\n",
    "\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm. The code ran but there is nothing to see. This is because we have no output! Let's get some output and see what it did. \n",
    "\n",
    "The next code block plays around with 'cl', the classifier we trained on our 'train' data set.\n",
    "\n",
    "The first line asks 'cl' to return a judgment of one sentence about a library. \n",
    "\n",
    "Then, we ask it to return a judgement of another sentence about something being a doozy. Although both times we get a judgement on whether the sentence is 'pos' or 'neg', the second one has more detailed sub-judgements we can analyse that show us how the positive and negative the sentence is so we can see whether the overall judgement is close or not. \n",
    "\n",
    "Do the Run/Shift+Enter thing that you are so good at doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our 'cl' classifier says 'This is an amazing library!' is  pos\n",
      "...\n",
      "Our 'cl' classifier says 'This one is a doozy.' is probably pos because its positive score is  0.63  and its negative score is  0.37 .\n"
     ]
    }
   ],
   "source": [
    "print(\"Our 'cl' classifier says 'This is an amazing library!' is \", cl.classify(\"This is an amazing library!\"))\n",
    "print('...')\n",
    "\n",
    "prob_dist = cl.prob_classify(\"This one is a doozy.\")\n",
    "print(\"Our 'cl' classifier says 'This one is a doozy.' is probably\",\n",
    "      prob_dist.max(), \"because its positive score is \",\n",
    "      round(prob_dist.prob(\"pos\"), 2),\n",
    "      \" and its negative score is \",\n",
    "      round(prob_dist.prob(\"neg\"), 2),\n",
    "      \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super. Now... What if we want to apply our 'cl' classifier to a document with multiple sentences... What kind of judgements can we get with that? \n",
    "\n",
    "Well, textblob is sophisticated enough to give an overall 'pos' or 'neg' judgement, as well as a sentence-by-sentence judgement. \n",
    "\n",
    "Run/Shift+Enter, buddy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall, 'blob' is  pos  because it's sentences are ...\n",
      "The beer is good.\n",
      "pos\n",
      "But the hangover is horrible.\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(\"The beer is good. But the hangover is horrible.\", classifier=cl)\n",
    "\n",
    "print(\"Overall, 'blob' is \", blob.classify(), \" because it's sentences are ...\")\n",
    "for s in blob.sentences:\n",
    "     print(s)\n",
    "     print(s.classify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we try to classify a document that we converted to Textblob format with the built-in sentiment analyser?\n",
    "\n",
    "Well, we still have Doc1 to try it on.\n",
    "\n",
    "Run/Shift+Enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textblob is just super. I love it!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "This blob has no classifier. Train one first!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e29ad803911a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDoc1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mDoc1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[1;34m\"\"\"Classify the blob using the blob's ``classifier``.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mNameError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"This blob has no classifier. Train one first!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: This blob has no classifier. Train one first!"
     ]
    }
   ],
   "source": [
    "print(Doc1)\n",
    "Doc1.classify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh huh. We get an error. \n",
    "\n",
    "The error message says the blob known as Doc1 has no classifier. It suggests we train one first, but we can just apply 'cl'. \n",
    "\n",
    "Run/Shift+Enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_Doc1 = TextBlob('Textblob is just super. I love it!', classifier=cl)\n",
    "cl_Doc1.classify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, when we classify the string that originally went into Doc1 using our 'cl' classifier, we still get a positive judgement. \n",
    "\n",
    "Now, what about accuracy? We have been using 'cl' even though it is trained on a REALLY tiny training data set. What does that do to our accuracy? For that, we need to run an accuracy challenge using our test data set. \n",
    "\n",
    "Run/Shift+Enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm. Not perfect.\n",
    "\n",
    "Fortunately, we can add more training data and try again. The code below defines a new training data set and then runs a re-training functiong called 'update' on our 'cl' classifier. \n",
    "\n",
    "Run/Shift+Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = [('She is my best friend.', 'pos'),\n",
    "            (\"I'm happy to have a new friend.\", 'pos'),\n",
    "            (\"Stay thirsty, my friend.\", 'pos'),\n",
    "            (\"He ain't from around here.\", 'neg')]\n",
    "\n",
    "cl.update(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, edit and run the next code block to test out how the new_data has improved 'cl. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy and paste the accuracy challenge from above into this cell and re-run it to get an updated accuracy score. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can train and test a sentiment analysis tool with more interesting data too..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all well and good, but seriously, 'cl' is trained on some seriously trivial data. What if we want to use some more interesting data, like the Doc_set that we imported from .csv earlier?\n",
    "\n",
    "Well, we are in luck! Sort of...\n",
    "\n",
    "We can definitely train a classifier on Doc_set, but let's just have a closer look at Doc_set before we jump right in and try that. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ï»¿\"@switchfoot http://twitpic.com/2y1zl - Awww', ' that\\'s a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"', '0'], [\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", '0'], ['@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds', '0'], ['my whole body feels itchy and like its on fire ', '0'], [\"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \", '0'], ['@Kwesidei not the whole crew ', '0'], ['Need a hug ', '0'], [\"@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\", '0'], [\"@Tatiana_K nope they didn't have it \", '0'], ['@twittera que me muera ? ', '0'], [\"spring break in plain city... it's snowing \", '0'], ['I just re-pierced my ears ', '0'], [\"@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .\", '0'], ['@octolinz16 It it counts, idk why I did either. you never talk to me anymore ', '0'], [\"@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.\", '0'], ['@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!', '0'], [\"Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?\", '0'], ['about to file taxes ', '0'], ['@LettyA ahh ive always wanted to see rent  love the soundtrack!!', '0'], ['@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks? ', '0']]\n",
      "...\n",
      "10002\n"
     ]
    }
   ],
   "source": [
    "print(Doc_set[:20])\n",
    "print('...')\n",
    "print(len(Doc_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc_set is a set of comments that come from product reviews. Each item has two strings, the first of which is the comment and the second of which is string with a number 4, 2 or 0. This second item, the string with a number inside, as a score of whether the comment is positive or negative. These scores may have been manually created, or may be the result of a semi-manual or supervised automation process. Excellent for our purposes, but not ideal because:\n",
    "- These scores are strings rather than integers. You can tell because they are enclose in quotes.\n",
    "- These scores range from 0 (negative) to 4 (positive) and also contains 2 (neutral), while the textblob sentiment analysis and classifier functions we have been using return scores from -1 (negative) through 0 (neutral) to 1 (positive). \n",
    "\n",
    "Well, we could change all the 4 to 1, 2 to 0 and 0 to -1 with RegEx if we wanted. But as you will see, this is not strictly necessary. \n",
    "\n",
    "However, there is another issue. Doc_set has over 10,000 items. This is big, but this is actually MUCH smaller than it could be. This is a subset of a 100,000 item data set that you can download for free. The original data set was way too big for jupyter notebook and was even too big for me to analyse on my laptop. I know because I tried. When you find yourself in a situation like this, you can try: \n",
    "- Accessing proper research computing facilities (good for real research, too much for a code demo). \n",
    "- Dividing a too big data set into into chunks, and train/update a chunk at a time. \n",
    "- Processing a too big data set to remove punctuation, stop words, urls, twitter handles, etc. (saving computer power for what matters).\n",
    "- Or a combination of these options. \n",
    "\n",
    "I already divided Doc_set into this subset, but I will also process that subset to remove irrelevancies. Since previous code demos in this series explained how to process imported data, I won't go into too much detail. \n",
    "\n",
    "First, download and import all the necessary things...\n",
    "Run/Shift+Enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\programdata\\anaconda3\\lib\\site-packages (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mzyssjkc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect\n",
    "from autocorrect import Speller\n",
    "check = Speller(lang='en')\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "English_punctuation = \"!\\\"#$%&()*+,./:;<=>?@[¿\\]--'^_`{|}~“”\"      \n",
    "table_punctuation = str.maketrans('','', English_punctuation)\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, process a subset of Doc_set and train a new classifier from it. \n",
    "\n",
    "Run/Shift+Enter, but be patient. This will take more time than most of the other code blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc_set_train_1 = []\n",
    "for thingy in Doc_set:\n",
    "    no_url = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', thingy[0], flags=re.MULTILINE)\n",
    "    x0 = word_tokenize(no_url)\n",
    "    x1 = [w.translate(table_punctuation) for w in x0]  \n",
    "    x2 = [word.lower() for word in x1]\n",
    "    x3 = [check(word) for word in x2]\n",
    "    x4 = list(filter(None, x3))  \n",
    "    x5 = ' '.join(x4)  \n",
    "    Doc_set_train_1.append([x5, thingy[1]])\n",
    "\n",
    "cl_Doc_set = NaiveBayesClassifier(Doc_set_train_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our 'cl' classifier says 'This is an amazing library!' is  pos\n",
      "...\n",
      "[['switchfoot http twitpiccom2y1l www that s a bummer you should got david carr of third day to do it d', '0'], ['is upset that he ca nt update his talebook by testing it and might cry as a result school today also blah', '0'], ['kenichan i dived many times for the ball managed to save 50 the rest go out of bounds', '0'], ['my whole body feels itchy and like its on fire', '0'], ['nationwideclass no it s not behaving at all i m mad why am i here because i ca nt see you all over there', '0'], ['kwesidei not the whole crew', '0'], ['need a hug', '0'], ['loutish hey long time no see yes rains a bit only a bit low i m fine thanks how s you', '0'], ['tatinek nope they did nt have it', '0'], ['twitter que me mwera', '0'], ['spring break in plain city it s snowing', '0'], ['i just pierced my ears', '0'], ['caregiving i could nt bear to watch it and i thought the a loss was embarrassing', '0'], ['octopine16 it it counts ink why i did either you never talk to me anymore', '0'], ['garrison i would ve been the first but i did nt have a gun not really though zac snider s just a doucheclown', '0'], ['iamjazzyfizzle i wish i got to watch it with you i miss you and iamlilnicki how was the premiere', '0'], ['hollos death scene will hurt me severely to watch on film wry is directors cut not out now', '0'], ['about to file taxes', '0'], ['letter ahh ive always wanted to see rent love the soundtrack', '0'], ['fakerpattypattz oh dear were you drinking out of the forgotten table drinks', '0']]\n",
      "Our 'cl_Doc_set' says the same sentence is  0\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(\"Our 'cl' classifier says 'This is an amazing library!' is \", cl.classify(\"This is an amazing library!\"))\n",
    "print('...')\n",
    "\n",
    "print(Doc_set_train_1[:20])\n",
    "print(\"Our 'cl_Doc_set' says the same sentence is \", cl_Doc_set.classify(\"This is an amazing library!\"))\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, no? Our 'cl' classifier returns 'pos' while our 'cl_Doc_set' returns '4' for the same sentence. In fact, the classifier does not care what categories you classify each item as, nor even how many classes there are. We can have two classes called 'pos' and 'neg' or three classes called '4', '2', and '0'. No worries. \n",
    "\n",
    "\n",
    "Just a note - too many classes for too few items in the training set will return poor accuracy. You want A WHOLE LOT of examples of each class if you want good judgements on class probability. \n",
    "\n",
    "Speaking of accuracy, how do we test our 'cl_Doc_set'? Well, fortunately, we can load a test set of product reviews too!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Sentiment_Analysis/test_min.csv', newline='') as f:              # 2, a csv of scored product reviews\n",
    "    reader = csv.reader(f)\n",
    "    Doc_test_set = list(reader)\n",
    "\n",
    "print(Doc_test_set[:20])   \n",
    "\n",
    "Doc_set_test_1 = []\n",
    "for thingy in Doc_set_test:\n",
    "    no_url = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', thingy[0], flags=re.MULTILINE)\n",
    "    x0 = word_tokenize(no_url)\n",
    "    x1 = [w.translate(table_punctuation) for w in x0]  \n",
    "    x2 = [word.lower() for word in x1]\n",
    "    x3 = [check(word) for word in x2]\n",
    "    x4 = list(filter(None, x3))  \n",
    "    x5 = ' '.join(x4)  \n",
    "    Doc_set_test_1.append([x5, thingy[1]])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc_set_corrected = []\n",
    "\n",
    "for pair in Doc_set:\n",
    "    x = []\n",
    "    x.append(pair[0])\n",
    "    if (pair[0][1] >= '4'):\n",
    "        x.append(1)\n",
    "    elif (pair[0][1] == '2'):\n",
    "        x.append(0)\n",
    "    else:\n",
    "        x.append(-1)\n",
    "    Doc_set_corrected.append(x)\n",
    "\n",
    "print(Doc_set_corrected[:20])\n",
    "\n",
    "Doc_set_scored = []\n",
    "for pair in Doc_set_corrected:\n",
    "    x=[]\n",
    "    Doc_set_scored.append([pair[0], pair[1], \n",
    "                           TextBlob(pair[0]).sentiment.polarity])\n",
    "    Doc_set_scored.append\n",
    "\n",
    "print(Doc_set_scored[:100])\n",
    "\n",
    "#### Calculate accuracy score\n",
    "Doc_set_accuracy =[]\n",
    "\n",
    "for item in Doc_set_scored:\n",
    "    x = item[1]\n",
    "    y = item[2]\n",
    "    Doc_set_accuracy.append(abs (x-y))\n",
    "\n",
    "print(statistics.mean(Doc_set_accuracy)) ##not a great result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have only started to dip our toes into what NLP can do, but hopefully this will whet your appetite to know more. \n",
    "\n",
    "As before, these exercises and this sample code should highlight to you that you need to think about:\n",
    "- your research questions and what you want to show, explore or understand, \n",
    "- your data, texts, corpus, or other research materials to analyse etc. \n",
    "- how your processes are related to your reserch questions, and \n",
    "- how your processes and data can be made available and reproducible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Books, tutorials, package recommendations, etc. for Python\n",
    "\n",
    "- Natural Language Processing with Python by Steven Bird, Ewan Klein and Edward Loper, http://www.nltk.org/book/\n",
    "- Foundations of Statistical Natural Language Processing by Christopher Manning and Hinrich Schütze, https://nlp.stanford.edu/fsnlp/promo/\n",
    "- Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition by Dan Jurafsky and James H. Martin, https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf\n",
    "- Deep Learning in Natural Language Processing by Li Deng, Yang Liu, https://lidengsite.wordpress.com/book-chapters/\n",
    "- Sentiment Analysis data sets https://blog.cambridgespark.com/50-free-machine-learning-datasets-sentiment-analysis-b9388f79c124\n",
    "\n",
    "NLTK options\n",
    "- nltk.corpus http://www.nltk.org/howto/corpus.html\n",
    "- Data Camp tutorial on sentiment analysis with nltk https://www.datacamp.com/community/tutorials/simplifying-sentiment-analysis-python\n",
    "- Vader sentiment analysis script available on github (nltk) https://www.nltk.org/_modules/nltk/sentiment/vader.html\n",
    "- TextBlob https://textblob.readthedocs.io/en/dev/\n",
    "- Flair, a NLP script available on github https://github.com/flairNLP/flair\n",
    "\n",
    "spaCy options\n",
    "- spaCy https://nlpforhackers.io/complete-guide-to-spacy/\n",
    "- Data Quest tutorial on sentiment analysis with spaCy https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\n",
    "\n",
    "\n",
    "Books and package recommendations for R\n",
    "- Quanteda, an R package for text analysis https://quanteda.io/​\n",
    "- Text Mining with R, a free online book https://www.tidytextmining.com/​"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
